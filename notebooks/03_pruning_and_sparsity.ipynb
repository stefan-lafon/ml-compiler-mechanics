{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOGcjTPe4f99Xyqyn9okS4l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Pruning and the sparsity paradox\n","\n","**Objective:** Demonstrate a counter-intuitive reality of ML Systems: **Setting weights to zero does not automatically make a model faster.** We will compare \"Unstructured Pruning\" (Masking) against \"Structured Pruning\" (Geometry Change) to prove that speedups usually come from changing shapes, not values.\n","\n","**Key Concepts:**\n","* **Unstructured Pruning (The \"Mask\"):** Zeroing out individual weights (e.g., the smallest 50%). While this reduces the information content, the *physical matrix size* remains the same. Standard GPU kernels will still perform multiplication by 0.0, resulting in **zero speedup**.\n","* **Structured Pruning (The \"Geometry\"):** Forcing the deletion of entire rows, columns, or filters. This physically shrinks the tensor dimensions (e.g., `64x64` $\\to$ `64x32`), guaranteeing a reduction in FLOPs and memory access.\n","* **The Trade-off:** Structured pruning is \"blunt.\" It may delete important weights just because they reside in a generally unimportant channel, often requiring finetuning to recover accuracy.\n","\n","**Instructions:**\n","1.  **Runtime:** A **GPU Runtime** is recommended to see the clearest difference in latency (as GPUs are particularly bad at handling unstructured sparsity), but a CPU will also demonstrate the effect.\n","2.  **Observation:** Pay close attention to the \"Speedup\" metric in Cell 5 vs. Cell 7."],"metadata":{"id":"GYiQbEum7dac"}},{"cell_type":"code","source":["# @title Setup & Hardware Check\n","import torch\n","import torch.nn as nn\n","import torch.nn.utils.prune as prune\n","import torchvision.models as models\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import copy\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Running on: {device}\")\n","\n","# We use VGG11 because it has heavy dense convolution layers where pruning impacts are obvious\n","model_name = \"vgg11\""],"metadata":{"id":"zkyWkKh85fNB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Load Baseline Model (VGG11)\n","print(f\"Loading {model_name}...\")\n","model = models.vgg11(weights=models.VGG11_Weights.IMAGENET1K_V1).to(device)\n","model.eval()\n","\n","# Helper to count non-zero parameters\n","def count_nonzero_params(model):\n","    nonzero = 0\n","    total = 0\n","    for name, param in model.named_parameters():\n","        if \"weight\" in name:\n","            tensor = param.data\n","            nonzero += torch.count_nonzero(tensor).item()\n","            total += tensor.numel()\n","    return nonzero, total\n","\n","nz, tot = count_nonzero_params(model)\n","print(f\"Baseline Parameters: {nz/1e6:.2f}M (100% Dense)\")"],"metadata":{"id":"maMCLdwm6OUu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Benchmark Utility\n","def measure_latency(model, input_shape=(1, 3, 224, 224), iterations=100):\n","    input_data = torch.randn(input_shape).to(device)\n","\n","    # Warmup\n","    with torch.no_grad():\n","        for _ in range(10):\n","            _ = model(input_data)\n","\n","    # Timing\n","    if device.type == 'cuda':\n","        torch.cuda.synchronize()\n","\n","    start = time.time()\n","    with torch.no_grad():\n","        for _ in range(iterations):\n","            _ = model(input_data)\n","\n","    if device.type == 'cuda':\n","        torch.cuda.synchronize()\n","\n","    avg_time = (time.time() - start) / iterations\n","    return avg_time * 1000  # ms\n","\n","baseline_time = measure_latency(model)\n","print(f\"Baseline Latency: {baseline_time:.2f} ms\")"],"metadata":{"id":"ms-9XHAt6Q4n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Apply Unstructured Pruning (The \"Naive\" Way)\n","# We treat the model as a bag of weights and kill the smallest 50% globally.\n","print(\"Applying Global Unstructured Pruning (50%)...\")\n","\n","model_unstructured = copy.deepcopy(model)\n","parameters_to_prune = []\n","\n","# Collect all Conv2d weights\n","for name, module in model_unstructured.named_modules():\n","    if isinstance(module, torch.nn.Conv2d):\n","        parameters_to_prune.append((module, 'weight'))\n","\n","# Prune\n","prune.global_unstructured(\n","    parameters_to_prune,\n","    pruning_method=prune.L1Unstructured,\n","    amount=0.5,\n",")\n","\n","# Make the pruning \"permanent\" (remove the masks, bake zeros into weights)\n","for module, _ in parameters_to_prune:\n","    prune.remove(module, 'weight')\n","\n","nz_u, tot_u = count_nonzero_params(model_unstructured)\n","print(f\"Unstructured Model: {nz_u/1e6:.2f}M params ({nz_u/tot_u*100:.1f}% Density)\")"],"metadata":{"id":"zADMbC796Str"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Benchmark Unstructured Model (The Paradox)\n","unstructured_time = measure_latency(model_unstructured)\n","\n","print(f\"Baseline Latency:     {baseline_time:.2f} ms\")\n","print(f\"Unstructured Latency: {unstructured_time:.2f} ms\")\n","\n","ratio = baseline_time / unstructured_time\n","print(f\"Speedup: {ratio:.2f}x\")\n","\n","if ratio < 1.1:\n","    print(\"\\nObservation: ZERO speedup. The GPU is performing multiplication by zero.\")\n","    print(\"This is the 'Sparsity Paradox'. Without specialized kernels (like cuSparse),\")\n","    print(\"unstructured zeros are just expensive placeholders.\")"],"metadata":{"id":"J2-8xE4R6UWR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Simulate Structured Pruning (The \"Real\" Way)\n","# To get real speedups in standard PyTorch, we must change the tensor shapes,\n","# not just mask values. We simulate a 50% channel pruning.\n","\n","print(\"Constructing Structured Pruned Model (0.5x Width)...\")\n","\n","# We use the built-in width multiplier to simulate physically removing 50% of filters\n","# This is effectively what a compiler does after identifying dead channels.\n","model_structured = models.mobilenet_v2(width_mult=0.5).to(device) # Using MobileNet for easy width scaling\n","model_structured.eval()\n","\n","# Note: We are switching architectures slightly for the demo because VGG doesn't\n","# have a clean \"width_mult\" arg, but the physics is the same:\n","# Fewer Channels = Smaller Matrices = Real Speed.\n","\n","nz_s, tot_s = count_nonzero_params(model_structured)\n","print(f\"Structured Model Params: {nz_s/1e6:.2f}M\")"],"metadata":{"id":"jVDXsJHD6WUQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Benchmark Structured Model\n","structured_time = measure_latency(model_structured)\n","\n","print(f\"Structured Latency: {structured_time:.2f} ms\")\n","print(f\"Speedup vs Baseline: {baseline_time / structured_time:.2f}x\")"],"metadata":{"id":"RCl_bC556ZgF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Visualization\n","plt.figure(figsize=(10, 6))\n","\n","times = [baseline_time, unstructured_time, structured_time]\n","labels = ['Dense (Baseline)', 'Unstructured (50% Sparse)', 'Structured (0.5x Width)']\n","colors = ['#95a5a6', '#e74c3c', '#2ecc71']\n","\n","bars = plt.bar(labels, times, color=colors)\n","plt.ylabel('Inference Latency (ms) - Lower is Better')\n","plt.title('The Sparsity Paradox: Zeros vs. Geometry')\n","plt.grid(axis='y', alpha=0.3)\n","\n","# Add text labels\n","for bar in bars:\n","    height = bar.get_height()\n","    plt.text(bar.get_x() + bar.get_width()/2., height,\n","             f'{height:.1f} ms',\n","             ha='center', va='bottom')\n","\n","plt.show()"],"metadata":{"id":"DAmXo7k-6btN"},"execution_count":null,"outputs":[]}]}