{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyP2sFIB4e5J8nrDd2/Y6iVv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Quantization and precision analysis\n","\n","**Objective:** Demonstrate how **TFLite Dynamic Range Quantization** reduces model footprint by 4x (from 32-bit float to 8-bit integer) while maintaining near-original accuracy.\n","\n","**Key Concepts:**\n","* **Dynamic Range Quantization:** Converting weights to INT8 to save space, while keeping activations in FP32 for computation stability.\n","* **Calibration:** Using a representative dataset to determine the dynamic range (min/max) of values.\n","* **Confidence Drift:** Measuring the divergence in probability scores between the original and quantized models.\n","\n","**Instructions:**\n","1.  **Runtime:** The standard **CPU Runtime** is sufficient (no GPU required).\n","2.  **Run:** Execute all cells to download calibration data, convert the model, and generate the degradation report."],"metadata":{"id":"cYgX6lsiHeUk"}},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"11wm8uGTDwTE"},"outputs":[],"source":["# @title 1. Setup\n","import tensorflow as tf\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","from skimage import data\n","from skimage.transform import resize\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n","\n","print(f\"TensorFlow Version: {tf.__version__}\")\n","\n","# MobileNetV2 requires 224x224 images\n","IMG_SIZE = 224"]},{"cell_type":"code","source":["# @title 2. Data Factory\n","def get_calibration_data():\n","    \"\"\"\n","    Loads standard computer vision images from skimage.\n","    Returns:\n","        raw_images: List of images for display (human readable)\n","        input_batch: Preprocessed numpy batch (model ready)\n","    \"\"\"\n","    # 1. Load standard images (guaranteed to exist in all versions)\n","    images = [\n","        data.chelsea(),      # Cat\n","        data.astronaut(),    # Human\n","        data.coffee(),       # Object\n","        data.rocket()        # Vehicle\n","    ]\n","\n","    # 2. Resize and Preprocess\n","    processed_imgs = []\n","    for img in images:\n","        # Resize to 224x224\n","        img_resized = resize(img, (IMG_SIZE, IMG_SIZE), preserve_range=True)\n","        # Expand dims to (1, 224, 224, 3)\n","        img_expanded = np.expand_dims(img_resized, axis=0)\n","        # Preprocess (MobileNetV2 specific: scales to [-1, 1])\n","        img_preproc = preprocess_input(img_expanded)\n","        processed_imgs.append(img_preproc)\n","\n","    # Stack into a single batch: (4, 224, 224, 3)\n","    input_batch = np.vstack(processed_imgs)\n","\n","    return images, input_batch\n","\n","# Load and verify\n","raw_images, test_batch = get_calibration_data()\n","\n","# Show them to the user\n","plt.figure(figsize=(12, 3))\n","for i, img in enumerate(raw_images):\n","    plt.subplot(1, 4, i+1)\n","    plt.imshow(img)\n","    plt.axis('off')\n","    plt.title(f\"Image {i+1}\")\n","plt.show()\n","\n","print(f\"Input Batch Shape: {test_batch.shape}\")"],"metadata":{"cellView":"form","id":"YWk5_cWfD2t8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 3. Load Baseline Model\n","print(\"Loading MobileNetV2 (FP32)...\")\n","baseline_model = tf.keras.applications.MobileNetV2(weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3))\n","\n","# Sanity Check: Run prediction on the Astronaut (Index 1)\n","print(\"\\n--- Baseline Sanity Check (Astronaut) ---\")\n","astro_pred = baseline_model.predict(test_batch[1:2], verbose=0)\n","print(decode_predictions(astro_pred, top=1)[0])"],"metadata":{"cellView":"form","id":"PGqzc-arEA48"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 4. Quantization Pipeline\n","\n","# Helper to save TFLite models\n","def save_tflite(tflite_model, filename):\n","    with open(filename, 'wb') as f:\n","        f.write(tflite_model)\n","    print(f\"Saved: {filename}\")\n","\n","# 1. Convert to Standard FP32 TFLite (No Optimization)\n","converter = tf.lite.TFLiteConverter.from_keras_model(baseline_model)\n","tflite_fp32 = converter.convert()\n","save_tflite(tflite_fp32, 'mobilenet_fp32.tflite')\n","\n","# 2. Convert to Dynamic Range Quantization (INT8 Weights)\n","# We set the optimization flag. TFLite will quantize weights to INT8\n","# but keep activations in FP32 during runtime.\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","tflite_quant = converter.convert()\n","save_tflite(tflite_quant, 'mobilenet_int8.tflite')"],"metadata":{"cellView":"form","id":"DHu1jvZAEFRI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 5. Size Benchmark\n","size_fp32 = os.path.getsize('mobilenet_fp32.tflite') / (1024 * 1024)\n","size_int8 = os.path.getsize('mobilenet_int8.tflite') / (1024 * 1024)\n","\n","print(f\"FP32 Model Size: {size_fp32:.2f} MB\")\n","print(f\"INT8 Model Size: {size_int8:.2f} MB\")\n","print(f\"Compression Ratio: {size_fp32 / size_int8:.2f}x\")"],"metadata":{"cellView":"form","id":"0hgpLoadEL1H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 6. Interpreter Helper\n","def run_tflite_inference(model_path, input_data):\n","    \"\"\"\n","    Runs inference on a TFLite model file.\n","    \"\"\"\n","    # Load Interpreter\n","    interpreter = tf.lite.Interpreter(model_path=model_path)\n","    interpreter.allocate_tensors()\n","\n","    # Get input/output details\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    # Run inference for each image in the batch\n","    outputs = []\n","    for i in range(len(input_data)):\n","        # Set input tensor (needs specific shape, e.g., (1, 224, 224, 3))\n","        input_tensor = input_data[i:i+1].astype(np.float32)\n","        interpreter.set_tensor(input_details[0]['index'], input_tensor)\n","\n","        # Run\n","        interpreter.invoke()\n","\n","        # Get output\n","        output_data = interpreter.get_tensor(output_details[0]['index'])\n","        outputs.append(output_data)\n","\n","    return np.vstack(outputs)"],"metadata":{"cellView":"form","id":"EtOioUjGEQoE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 7. Accuracy Evaluation\n","print(\"Running Inference on Test Batch...\")\n","\n","# 1. Run both models\n","# Note: Ensure you ran Cell 6 to define 'run_tflite_inference' first!\n","preds_fp32 = run_tflite_inference('mobilenet_fp32.tflite', test_batch)\n","preds_int8 = run_tflite_inference('mobilenet_int8.tflite', test_batch)\n","\n","# 2. Calculate Degradation\n","results = []\n","\n","# Dynamic loop based on actual batch size (safe for 4 or 5 images)\n","num_images = len(test_batch)\n","\n","for i in range(num_images):\n","    # Get top prediction index and confidence\n","    top_fp32 = np.argmax(preds_fp32[i])\n","    conf_fp32 = preds_fp32[i][top_fp32]\n","\n","    top_int8 = np.argmax(preds_int8[i])\n","    conf_int8 = preds_int8[i][top_int8]\n","\n","    # Calculate Mean Squared Error between the full probability vectors\n","    mse = np.mean((preds_fp32[i] - preds_int8[i])**2)\n","\n","    results.append({\n","        \"image_idx\": i,\n","        \"label_match\": top_fp32 == top_int8,\n","        \"conf_delta\": conf_fp32 - conf_int8,\n","        \"mse\": mse,\n","        \"top_fp32\": top_fp32,\n","        \"conf_fp32\": conf_fp32,\n","        \"conf_int8\": conf_int8\n","    })\n","\n","# Print Summary\n","print(\"\\n--- Degradation Report ---\")\n","for res in results:\n","    status = \" MATCH\" if res['label_match'] else \" FLIP \"\n","    print(f\"Img {res['image_idx']}: {status} | Conf Drift: {res['conf_delta']:.4f} | MSE: {res['mse']:.6f}\")"],"metadata":{"cellView":"form","id":"awoiwlp8FMCM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 8. Visualization\n","from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n","\n","fig = plt.figure(figsize=(14, 6))\n","\n","# --- Plot 1: Size Comparison (Unchanged) ---\n","ax1 = plt.subplot(1, 2, 1)\n","bars1 = ax1.bar(['FP32 (Original)', 'INT8 (Quantized)'], [size_fp32, size_int8], color=['#3498db', '#e74c3c'])\n","ax1.set_title('Model Footprint (Lower is Better)', fontsize=12)\n","ax1.set_ylabel('Size (MB)')\n","ax1.grid(axis='y', alpha=0.3)\n","for bar in bars1:\n","    height = bar.get_height()\n","    ax1.text(bar.get_x() + bar.get_width()/2., height,\n","             f'{height:.1f} MB', ha='center', va='bottom', fontsize=10)\n","\n","# --- Plot 2: Confidence Drift with Thumbnails ---\n","ax2 = plt.subplot(1, 2, 2)\n","drifts = [r['conf_delta'] * 100 for r in results] # Convert to %\n","colors = ['green' if d <= 0 else 'orange' for d in drifts]\n","x_pos = range(1, len(results) + 1)\n","\n","bars2 = ax2.bar(x_pos, drifts, color=colors)\n","ax2.axhline(0, color='black', linewidth=0.8)\n","ax2.set_title('Confidence Loss per Image (Lower is Better)', fontsize=12)\n","ax2.set_xlabel('Test Image', fontsize=10)\n","ax2.set_ylabel('Confidence Drop (%)', fontsize=10)\n","ax2.grid(axis='y', alpha=0.3)\n","ax2.set_xticks(x_pos)\n","\n","# Helper function to create thumbnail\n","def create_thumbnail(img_array, zoom=0.12):\n","    return OffsetImage(img_array, zoom=zoom)\n","\n","# Add images on top/bottom of bars\n","for i, bar in enumerate(bars2):\n","    height = bar.get_height()\n","\n","    # 1. Create the image box from raw_images data\n","    imagebox = create_thumbnail(raw_images[i])\n","\n","    # 2. Determine position\n","    # x center of bar, y at the top/bottom edge of bar\n","    xy = (bar.get_x() + bar.get_width() / 2, height)\n","\n","    # 3. Determine alignment based on positive/negative bar\n","    # If positive drop (orange), put image ABOVE bar (bottom aligned at (0.5, 0))\n","    # If negative drop (green), put image BELOW bar (top aligned at (0.5, 1))\n","    xybox_offset = (0, 5) if height >= 0 else (0, -5)\n","    alignment = (0.5, 0) if height >= 0 else (0.5, 1)\n","\n","    # 4. Create annotation\n","    ab = AnnotationBbox(imagebox, xy,\n","                        xybox=xybox_offset,\n","                        xycoords='data',\n","                        boxcoords=\"offset points\",\n","                        pad=0.1,\n","                        frameon=True,\n","                        bboxprops=dict(edgecolor=colors[i], lw=2), # Color border to match bar\n","                        box_alignment=alignment)\n","    ax2.add_artist(ab)\n","\n","# Manually expand y-limits to make room for images so they aren't cut off\n","y_min, y_max = ax2.get_ylim()\n","ax2.set_ylim(y_min - 2, y_max + 2)\n","\n","plt.tight_layout()\n","# Optional: save the high-quality version for the README immediately\n","plt.savefig('quantization_results_with_images.png', dpi=100, bbox_inches='tight')\n","print(\"Saved quantization_results_with_images.png\")\n","plt.show()"],"metadata":{"cellView":"form","id":"s0o-TsLjEVS5"},"execution_count":null,"outputs":[]}]}