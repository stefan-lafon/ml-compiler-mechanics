{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","private_outputs":true,"authorship_tag":"ABX9TyOtqKm6BLMI86riR65xnsTT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Fusion Lab: JAX Operator Fusion & Bandwidth Analysis\n","\n","**Objective:** Demonstrate how JAX's **XLA Compiler (JIT)** fuses element-wise operations into single kernels to overcome memory bottlenecks.\n","\n","**Key Concepts:**\n","* **Memory Bandwidth:** The speed at which data moves between VRAM and the Compute Units.\n","* **Operator Fusion:** Merging multiple math steps (e.g., sin -> cos -> add) into one loop to avoid reading/writing to VRAM repeatedly.\n","* **Arithmetic Intensity (The \"Workload\" Checkboxes):**\n","    * *Light:* Low intensity. The GPU is idle waiting for memory.\n","    * *Heavy:* High intensity. The GPU is busy calculating, hiding memory latency.\n","\n","**Instructions:**\n","1.  **Check Runtime:** Ensure you are using a GPU (Runtime > Change runtime type > T4 GPU).\n","2.  **Select Workloads:** Use the checkboxes to compare Low vs. High Arithmetic Intensity.\n","3.  **Run:** Click the **Run Benchmarks** button to see the Roofline plots."],"metadata":{"id":"tdn5T2nh26yE"}},{"cell_type":"code","source":["# @title Fusion Lab\n","\n","import jax\n","import jax.numpy as jnp\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import ipywidgets as widgets\n","from IPython.display import display, clear_output\n","\n","# --- Configuration ---\n","# Increasing this range to ensure we actually hit VRAM limits on larger GPUs\n","SIZES = [1024 * (2**i) for i in range(16)]\n","key = jax.random.key(0)\n","\n","# --- Kernel Definitions ---\n","# We want to compare low arithmetic intensity vs high intensity to see\n","# where the memory bandwidth bottleneck actually hits.\n","\n","def simple_scale(x):\n","    # Memory bound: barely any math, mostly reading/writing\n","    return x * 0.5 + 1.0\n","\n","def activation_mix(x):\n","    # Standard DL workload: mix of trig and basic arithmetic\n","    return jnp.tanh(x) * jnp.sin(x) + (x * 0.5)\n","\n","def heavy_shader(x):\n","    # Compute bound: forcing the ALU to work hard to hide memory latency\n","    y = x\n","    for _ in range(5):\n","        y = jnp.sin(y) * jnp.cos(y) + jnp.tanh(y)\n","    return y\n","\n","kernels = {\n","    \"Light (Memory Bound)\": simple_scale,\n","    \"Medium (Standard)\": activation_mix,\n","    \"Heavy (Compute Bound)\": heavy_shader\n","}\n","\n","# --- UI Setup ---\n","print(\"Initializing Dashboard...\")\n","\n","# Checkboxes for selecting which kernels to plot\n","checkboxes = {\n","    name: widgets.Checkbox(value=True, description=name, indent=False)\n","    for name in kernels\n","}\n","checkbox_ui = widgets.HBox(list(checkboxes.values()))\n","\n","# The big run button\n","btn_run = widgets.Button(\n","    description=\"Run Profiler\",\n","    button_style='success',\n","    icon='rocket' # classic dev humor\n",")\n","output_area = widgets.Output()\n","\n","# --- Benchmarking Logic ---\n","def run_benchmark(b):\n","    with output_area:\n","        clear_output(wait=True)\n","\n","        # quick hardware check to manage expectations\n","        backend = jax.default_backend()\n","        device = jax.devices()[0].device_kind\n","        print(f\"Running on: {device} ({backend.upper()})\")\n","\n","        if backend == 'cpu':\n","            print(\"Note: You're on CPU. You won't see the roofline effect clearly.\")\n","            print(\"Tip: Switch runtime to T4 GPU for the actual bandwidth test.\")\n","        else:\n","            print(\"GPU Active. Bandwidth saturation test ready.\")\n","        print(\"-\" * 40)\n","\n","        # Filter enabled tests\n","        active_tests = [k for k, cb in checkboxes.items() if cb.value]\n","        if not active_tests:\n","            print(\"Select a workload first!\")\n","            return\n","\n","        # Setup plots\n","        # squeeze=False ensures 'axes' is ALWAYS a 2D array, preventing index errors\n","        fig, axes = plt.subplots(len(active_tests), 1, figsize=(10, 5 * len(active_tests)), squeeze=False)\n","\n","        for idx, test_name in enumerate(active_tests):\n","            ax = axes[idx, 0]\n","            func = kernels[test_name]\n","            jit_func = jax.jit(func)\n","\n","            print(f\"Profiling: {test_name}...\")\n","\n","            data_mb = []\n","            bw_eager = []\n","            bw_jit = []\n","\n","            for n in SIZES:\n","                # Generate random data\n","                dim = int(np.sqrt(n))\n","                x = jax.random.normal(key, (dim, dim))\n","\n","                # total read + write bytes (approx)\n","                total_bytes = (n * 4) * 2\n","\n","                # 1. Eager Execution\n","                # CRITICAL: warm up call to block async dispatch\n","                _ = func(x[:2,:2]).block_until_ready()\n","\n","                t0 = time.time()\n","                func(x).block_until_ready()\n","                t_eager = time.time() - t0\n","\n","                # 2. JIT (Fused) Execution\n","                # Compile/Warmup first! Otherwise we time compilation.\n","                _ = jit_func(x[:2,:2]).block_until_ready()\n","\n","                t0 = time.time()\n","                jit_func(x).block_until_ready()\n","                t_jit = time.time() - t0\n","\n","                # Log stats\n","                data_mb.append((n*4) / 1e6) # size in MB\n","                bw_eager.append((total_bytes / t_eager) / 1e9) # GB/s\n","                bw_jit.append((total_bytes / t_jit) / 1e9)     # GB/s\n","\n","            # Visualization\n","            ax.plot(data_mb, bw_jit, 'o-', c='green', label='JIT (Fused)')\n","            ax.plot(data_mb, bw_eager, 'o--', c='red', label='Eager (Unfused)')\n","\n","            ax.set_xscale('log')\n","            ax.set_ylabel('Throughput (GB/s)')\n","            ax.set_title(f\"Roofline Analysis: {test_name}\")\n","            ax.grid(True, which=\"both\", alpha=0.3)\n","            ax.legend()\n","\n","            if idx == len(active_tests) - 1:\n","                ax.set_xlabel('Tensor Size (MB)')\n","\n","        print(\"Benchmark Complete.\")\n","        plt.tight_layout()\n","        plt.show()\n","\n","# Hook up events\n","btn_run.on_click(run_benchmark)\n","\n","display(widgets.VBox([checkbox_ui, btn_run]))\n","display(output_area)"],"metadata":{"id":"aX05zVVE0TDF"},"execution_count":null,"outputs":[]}]}