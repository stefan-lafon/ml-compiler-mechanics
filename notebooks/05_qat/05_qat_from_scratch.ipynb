{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","private_outputs":true,"authorship_tag":"ABX9TyN2ncdGUDmAlJtKm0gaxc7s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Module 5: Quantization Aware Training (QAT) from Scratch\n","\n","**Objective:**\n","In Module 4, we saw that certain \"Diva\" layers break when quantized to INT4. Standard Post-Training Quantization (PTQ) fails here because the weights were never trained to survive the \"rounding error.\"\n","\n","**The Problem: The Vanishing Gradient**\n","We cannot simply add `x = round(x)` to our training loop. The mathematical derivative of the round function is **0** everywhere (flat steps). If we use it, gradients die, and backpropagation stops.\n","\n","**The Solution: The Straight Through Estimator (STE)**\n","We will implement the industry-standard \"hack\" to bypass this:\n","1.  **Forward Pass:** Apply the rounding ($x_q = \\text{round}(x)$) so the model feels the noise.\n","2.  **Backward Pass:** Lie to the optimizer. We pretend the function was Identity ($y=x$), allowing gradients to flow \"straight through\" the quantization block.\n","\n","**Goal:**\n","Compare a **Naive PTQ** approach (Train FP32 -> Quantize) vs. **QAT** (Train Quantized) on a challenging INT4 regression task."],"metadata":{"id":"qgV_-H15YZhk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5k9UlTOYUOX"},"outputs":[],"source":["# @title Setup & Synthetic Data\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Set seed for reproducibility\n","torch.manual_seed(42)\n","\n","# Generate noisy sine wave data\n","# Task: Regress y = sin(x)\n","X = torch.linspace(-3, 3, 200).unsqueeze(1) # Inputs\n","y = torch.sin(X) + torch.randn(X.size()) * 0.1 # Targets with noise\n","\n","# Split into Train/Test\n","indices = torch.randperm(len(X))\n","X_train, y_train = X[indices[:160]], y[indices[:160]]\n","X_test, y_test = X[indices[160:]], y[indices[160:]]\n","\n","plt.figure(figsize=(10, 5))\n","plt.scatter(X_train, y_train, s=10, label='Training Data')\n","plt.plot(X, torch.sin(X), color='red', alpha=0.5, label='True Function')\n","plt.title(\"The Task: Fit the Sine Wave\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","source":["# @title The Straight Through Estimator (STE)\n","class STEQuantize(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, input, scale, zero_point, qmin, qmax):\n","        \"\"\"\n","        Forward: Quantize the input (Discrete).\n","        \"\"\"\n","        # Save metadata if needed for backward (not needed for simple STE)\n","        # ctx.save_for_backward(input)\n","\n","        # 1. Quantize\n","        q_input = (input / scale + zero_point).round().clamp(qmin, qmax)\n","\n","        # 2. Dequantize (Fake Quantization)\n","        dq_input = (q_input - zero_point) * scale\n","\n","        return dq_input\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        \"\"\"\n","        Backward: Pass the gradient straight through (Continuous).\n","        We return a gradient for every input argument.\n","        Only 'input' needs a real gradient; others are scalars (None).\n","        \"\"\"\n","        # THE LIE: We pretend the forward pass was Identity.\n","        # grad_input = grad_output\n","\n","        # Refinement: Clip gradients where input was clipped?\n","        # (Simple STE ignores clipping for stability, but we pass it through)\n","        return grad_output, None, None, None, None\n","\n","# Helper wrapper to use it like a function\n","def ste_quantize(x, num_bits=4):\n","    # Calculate simple Min-Max scale\n","    qmin = -(2**(num_bits - 1))\n","    qmax = (2**(num_bits - 1)) - 1\n","\n","    min_val, max_val = x.min(), x.max()\n","\n","    # Avoid div-by-zero\n","    if min_val == max_val:\n","        return x\n","\n","    scale = (max_val - min_val) / (qmax - qmin)\n","    zero_point = qmin - min_val / scale\n","\n","    return STEQuantize.apply(x, scale, zero_point, qmin, qmax)"],"metadata":{"id":"kjKYVAqiYdwq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title The QAT Layer (Linear + STE)\n","class QATLinear(nn.Linear):\n","    def __init__(self, in_features, out_features, num_bits=4):\n","        super().__init__(in_features, out_features)\n","        self.num_bits = num_bits\n","\n","    def forward(self, input):\n","        # 1. Quantize Weights (Training the model to deal with jagged weights)\n","        w_quant = ste_quantize(self.weight, self.num_bits)\n","\n","        # 2. Quantize Input (Optional: Activation Quantization)\n","        # For simplicity, we focus on Weight QAT here.\n","        # i_quant = ste_quantize(input, self.num_bits)\n","\n","        # 3. Linear Operation using Quantized Weights\n","        return F.linear(input, w_quant, self.bias)\n","\n","import torch.nn.functional as F\n","print(\"QAT Linear Layer defined.\")"],"metadata":{"id":"mWu7MaxHYf6m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Define Models: Baseline vs. QAT\n","# A simple MLP: 1 -> 64 -> 64 -> 1\n","\n","# 1. Standard FP32 Model\n","model_fp32 = nn.Sequential(\n","    nn.Linear(1, 64),\n","    nn.ReLU(),\n","    nn.Linear(64, 64),\n","    nn.ReLU(),\n","    nn.Linear(64, 1)\n",")\n","\n","# 2. QAT Model (Identical architecture, but with QATLinear)\n","model_qat = nn.Sequential(\n","    QATLinear(1, 64, num_bits=4),\n","    nn.ReLU(),\n","    QATLinear(64, 64, num_bits=4),\n","    nn.ReLU(),\n","    QATLinear(64, 1, num_bits=4)\n",")\n","\n","print(\"Models initialized.\")"],"metadata":{"id":"QIUSiHirYiGl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Train Both Models\n","def train(model, name, epochs=500):\n","    optimizer = optim.Adam(model.parameters(), lr=0.01)\n","    criterion = nn.MSELoss()\n","    losses = []\n","\n","    model.train()\n","    for epoch in range(epochs):\n","        optimizer.zero_grad()\n","        preds = model(X_train)\n","        loss = criterion(preds, y_train)\n","        loss.backward()\n","        optimizer.step()\n","        losses.append(loss.item())\n","\n","    return losses\n","\n","print(\"Training FP32 Baseline...\")\n","losses_fp32 = train(model_fp32, \"FP32\")\n","\n","print(\"Training QAT Model (INT4)...\")\n","losses_qat = train(model_qat, \"QAT\")\n","\n","plt.plot(losses_fp32, label='FP32 Loss')\n","plt.plot(losses_qat, label='QAT Loss')\n","plt.yscale('log')\n","plt.legend()\n","plt.title(\"Training Convergence\")\n","plt.show()"],"metadata":{"id":"9NmJAWM6YjuV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Final Benchmark: PTQ vs QAT\n","def evaluate(model, X, y, quantize_weights=False):\n","    model.eval()\n","    with torch.no_grad():\n","        # If testing PTQ, we must manually quantize the FP32 weights now\n","        if quantize_weights:\n","            for module in model.modules():\n","                if isinstance(module, nn.Linear):\n","                    module.weight.data = ste_quantize(module.weight.data, num_bits=4)\n","\n","        preds = model(X)\n","        loss = nn.MSELoss()(preds, y)\n","    return loss.item(), preds\n","\n","# 1. Evaluate FP32 (Golden)\n","loss_fp32, preds_fp32 = evaluate(model_fp32, X_test, y_test, quantize_weights=False)\n","\n","# 2. Evaluate PTQ (Take FP32 model -> Crush to INT4)\n","import copy\n","model_ptq = copy.deepcopy(model_fp32)\n","loss_ptq, preds_ptq = evaluate(model_ptq, X_test, y_test, quantize_weights=True)\n","\n","# 3. Evaluate QAT (Already trained with quantization)\n","loss_qat, preds_qat = evaluate(model_qat, X_test, y_test, quantize_weights=False)\n","\n","print(f\"MSE Loss Comparison (Lower is Better):\")\n","print(f\"1. FP32 Baseline:  {loss_fp32:.5f}\")\n","print(f\"2. Naive PTQ:      {loss_ptq:.5f}  (Usually fails)\")\n","print(f\"3. Trained QAT:    {loss_qat:.5f}  (Should recover)\")\n","\n","# --- VISUALIZATION FIX ---\n","# Sort data by X value so the plot lines draw smoothly from left to right\n","sort_indices = X_test.flatten().argsort()\n","\n","X_sorted = X_test[sort_indices]\n","y_sorted = y_test[sort_indices]\n","fp32_sorted = preds_fp32[sort_indices]\n","ptq_sorted = preds_ptq[sort_indices]\n","qat_sorted = preds_qat[sort_indices]\n","\n","plt.figure(figsize=(10, 6))\n","plt.scatter(X_sorted, y_sorted, color='gray', alpha=0.5, label='Test Data')\n","\n","plt.plot(X_sorted, fp32_sorted, label='FP32 (Golden)', linestyle='--')\n","plt.plot(X_sorted, ptq_sorted, label='Naive PTQ (Failure)', color='red', alpha=0.7)\n","plt.plot(X_sorted, qat_sorted, label='QAT (Success)', color='green', linewidth=2.5)\n","\n","plt.title(\"Visualizing the Recovery: PTQ vs QAT\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"hwU59yzqYh9p","cellView":"form"},"execution_count":null,"outputs":[]}]}